{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254dfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Dataset, DataLoader, InMemoryDataset\n",
    "import numpy as np\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb7557d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[236, 186],\n",
       "       [122, 285],\n",
       "       [ 24, 346],\n",
       "       ...,\n",
       "       [  0, 345],\n",
       "       [  0, 346],\n",
       "       [  0, 347]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_edge(file_path):\n",
    "    edges = []\n",
    "    with open(file_path) as file:\n",
    "        for line in file:\n",
    "            edge = line.strip().split()  # 根据文件格式解析边的信息\n",
    "            edges.append((int(edge[0]), int(edge[1])))  # 将边的起始节点和目标节点添加到列表中\n",
    "        for a in range(1,348):\n",
    "            edges.append([0,a])\n",
    "        return np.array(edges)\n",
    "edges=read_edge(r'facebook\\0.edges')\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7d1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從文件中讀取節點特徵向量\n",
    "def read_node_features(ego_file_path,file_path):\n",
    "    node_features = []\n",
    "    with open(ego_file_path) as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            # 根據文件格式擷取節點特徵向量\n",
    "            features = line.strip().split()\n",
    "            features = [float(f) for f in features]\n",
    "            node_features.append(features)\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "        a=1\n",
    "        for line in lines:\n",
    "            # 根據文件格式擷取節點特徵向量\n",
    "            features = line.strip(str(a)).split()\n",
    "            features = [float(f) for f in features]\n",
    "            node_features.append(features)\n",
    "            a=a+1\n",
    "        return np.array(node_features)\n",
    "\n",
    "# 讀取節點特徵向量數據\n",
    "node_features = read_node_features(r'facebook\\0.egofeat',r'facebook\\0.feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a12f745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[236, 122,  24,  ...,   0,   0,   0],\n",
      "        [186, 285, 346,  ..., 345, 346, 347]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(node_features, dtype=torch.float)  # node features\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # edge關係\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "print(data.edge_index)\n",
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "333c06f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3661813f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[236, 122,  24,  ...,   0,   0,   0],\n",
       "        [186, 285, 346,  ..., 345, 346, 347]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd994d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#獲得edge_features\n",
    "# generate dummy edge features (5-dimensional vector of ones).\n",
    "n_edge_channels = 5\n",
    "edge_features = torch.ones([data.edge_index.shape[1], n_edge_channels])\n",
    "edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da8dfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[348, 224], edge_index=[2, 5385], edge_attr=[5385, 5])\n"
     ]
    }
   ],
   "source": [
    "#建立好圖\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_features)\n",
    "data_backup = Data(x=x, edge_index=edge_index, edge_attr=edge_features)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a461bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9c5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = NeighborSampler(data.edge_index, sizes=[10, 10], batch_size=256,\n",
    "                               shuffle=True, num_nodes=data.num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c9ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "054a6d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 215, 54, 61, 298, 229, 81, 253, 193, 97, 264, 29, 132, 110, 163, 259, 183, 334, 245, 222, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yi-Chen\\AppData\\Local\\Temp/ipykernel_3020/31169211.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.array(interested_nodes)\n"
     ]
    }
   ],
   "source": [
    "#讀取子圖(circles)的節點\n",
    "interested_nodes = []\n",
    "with open(r'facebook\\0.circles') as file:\n",
    "    lines = file.readlines()\n",
    "    a=0\n",
    "    for line in lines:\n",
    "        # 根據文件格式擷取circle節點\n",
    "        b=\"circle\"+str(a)\n",
    "        features = line.strip(str(b)).split()\n",
    "        features = [int(f) for f in features]\n",
    "        features.append(0) #把egonode也加進circle裡面\n",
    "#         print(features)\n",
    "        interested_nodes.append(features)\n",
    "        a=a+1\n",
    "np.array(interested_nodes)\n",
    "print(interested_nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86fa8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph(a): #a代表第a個子圖(circle)\n",
    "    #建立egonode=0的subgraph\n",
    "    b=0\n",
    "    for b in range(1):#跑一次\n",
    "        SUB_G=[]\n",
    "        # 完整的圖數據中獲取感興趣節點和對應的邊索引\n",
    "        subgraph_nodes = data.x[interested_nodes[a]]\n",
    "        #把子圖節點索引轉成tensor\n",
    "        interested_nodes_tensor = torch.tensor(interested_nodes[a])\n",
    "        # Filter the edges based on the interested nodes\n",
    "        mask = torch.logical_or(torch.isin(data.edge_index[0], interested_nodes_tensor),\n",
    "                                torch.isin(data.edge_index[1], interested_nodes_tensor))\n",
    "        #得到子圖的edge index\n",
    "        SUB_E = data.edge_index[:, mask]\n",
    "        #得到子圖的edge attr\n",
    "        subgraph_edge_attr = data.edge_attr[torch.logical_or(\n",
    "        torch.isin(data.edge_index[0], interested_nodes_tensor),\n",
    "        torch.isin(data.edge_index[1], interested_nodes_tensor))]\n",
    "\n",
    "        # 建構子圖 data\n",
    "        SUB_G = Data(x=subgraph_nodes, edge_index=SUB_E, edge_attr=subgraph_edge_attr)\n",
    "        print(\"第\",a,\"個subgraph data:\")\n",
    "        print(SUB_G)\n",
    "        return SUB_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0004d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 個subgraph data:\n",
      "Data(x=[21, 224], edge_index=[2, 581], edge_attr=[581, 5])\n",
      "第 1 個subgraph data:\n",
      "Data(x=[2, 224], edge_index=[2, 357], edge_attr=[357, 5])\n",
      "第 2 個subgraph data:\n",
      "Data(x=[10, 224], edge_index=[2, 491], edge_attr=[491, 5])\n",
      "第 3 個subgraph data:\n",
      "Data(x=[4, 224], edge_index=[2, 377], edge_attr=[377, 5])\n",
      "第 4 個subgraph data:\n",
      "Data(x=[18, 224], edge_index=[2, 1007], edge_attr=[1007, 5])\n",
      "第 5 個subgraph data:\n",
      "Data(x=[2, 224], edge_index=[2, 379], edge_attr=[379, 5])\n",
      "第 6 個subgraph data:\n",
      "Data(x=[21, 224], edge_index=[2, 657], edge_attr=[657, 5])\n",
      "第 7 個subgraph data:\n",
      "Data(x=[3, 224], edge_index=[2, 373], edge_attr=[373, 5])\n",
      "第 8 個subgraph data:\n",
      "Data(x=[2, 224], edge_index=[2, 349], edge_attr=[349, 5])\n",
      "第 9 個subgraph data:\n",
      "Data(x=[11, 224], edge_index=[2, 443], edge_attr=[443, 5])\n",
      "第 10 個subgraph data:\n",
      "Data(x=[5, 224], edge_index=[2, 397], edge_attr=[397, 5])\n",
      "第 11 個subgraph data:\n",
      "Data(x=[31, 224], edge_index=[2, 1483], edge_attr=[1483, 5])\n",
      "第 12 個subgraph data:\n",
      "Data(x=[2, 224], edge_index=[2, 365], edge_attr=[365, 5])\n",
      "第 13 個subgraph data:\n",
      "Data(x=[6, 224], edge_index=[2, 405], edge_attr=[405, 5])\n",
      "第 14 個subgraph data:\n",
      "Data(x=[3, 224], edge_index=[2, 405], edge_attr=[405, 5])\n",
      "第 15 個subgraph data:\n",
      "Data(x=[134, 224], edge_index=[2, 3919], edge_attr=[3919, 5])\n",
      "第 16 個subgraph data:\n",
      "Data(x=[33, 224], edge_index=[2, 1069], edge_attr=[1069, 5])\n",
      "第 17 個subgraph data:\n",
      "Data(x=[10, 224], edge_index=[2, 475], edge_attr=[475, 5])\n",
      "第 18 個subgraph data:\n",
      "Data(x=[2, 224], edge_index=[2, 367], edge_attr=[367, 5])\n",
      "第 19 個subgraph data:\n",
      "Data(x=[14, 224], edge_index=[2, 625], edge_attr=[625, 5])\n",
      "第 20 個subgraph data:\n",
      "Data(x=[7, 224], edge_index=[2, 367], edge_attr=[367, 5])\n",
      "第 21 個subgraph data:\n",
      "Data(x=[2, 224], edge_index=[2, 347], edge_attr=[347, 5])\n",
      "第 22 個subgraph data:\n",
      "Data(x=[2, 224], edge_index=[2, 349], edge_attr=[349, 5])\n",
      "第 23 個subgraph data:\n",
      "Data(x=[4, 224], edge_index=[2, 407], edge_attr=[407, 5])\n"
     ]
    }
   ],
   "source": [
    "#建立所有子圖\n",
    "data1=subgraph(0)\n",
    "data2=subgraph(1)\n",
    "data3=subgraph(2)\n",
    "data4=subgraph(3)\n",
    "data5=subgraph(4)\n",
    "data6=subgraph(5)\n",
    "data7=subgraph(6)\n",
    "data8=subgraph(7)\n",
    "data9=subgraph(8)\n",
    "data10=subgraph(9)\n",
    "data11=subgraph(10)\n",
    "data12=subgraph(11)\n",
    "data13=subgraph(12)\n",
    "data14=subgraph(13)\n",
    "data15=subgraph(14)\n",
    "data16=subgraph(15)\n",
    "data17=subgraph(16)\n",
    "data18=subgraph(17)\n",
    "data19=subgraph(18)\n",
    "data20=subgraph(19)\n",
    "data21=subgraph(20)\n",
    "data22=subgraph(21)\n",
    "data23=subgraph(22)\n",
    "data24=subgraph(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dc1191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[21, 224], edge_index=[2, 581], edge_attr=[581, 5])\n"
     ]
    }
   ],
   "source": [
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce857e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca2516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['.\\MyOwn.dataset']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = [data1,data2,data3,data4,data5,data6,data7,data8,data9,data10]\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb92ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=MyOwnDataset('C:\\\\Users\\\\Yi-Chen\\\\python\\\\Final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48015bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle()\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fff407b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分割資料集\n",
    "train_dataset = dataset[:7]\n",
    "val_dataset = dataset[7:8]\n",
    "test_dataset = dataset[8:]\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84571be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34b04b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6cfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f185f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from typing import Union, Tuple\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size, OptTensor\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_sparse import SparseTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1370919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraFrankConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Modality-specific neighbor aggregation in GraFrank implemented by stacking message-passing layers that are\n",
    "    parameterized by friendship attentions over individual node features and pairwise link features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, normalize: bool = False,\n",
    "                 bias: bool = True, **kwargs):  # yapf: disable\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super(GraFrankConv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.negative_slope = 0.2\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.self_linear = nn.Linear(in_channels[1], out_channels, bias=bias)\n",
    "        self.message_linear = nn.Linear(in_channels[0], out_channels, bias=bias)\n",
    "\n",
    "        self.attn = nn.Linear(out_channels, 1, bias=bias)\n",
    "        self.attn_i = nn.Linear(out_channels, 1, bias=bias)\n",
    "\n",
    "        self.lin_l = nn.Linear(out_channels, out_channels, bias=bias)\n",
    "        self.lin_r = nn.Linear(out_channels, out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.dropout = 0\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_attr: OptTensor = None,\n",
    "                size: Size = None) -> Tensor:\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        x_l, x_r = x[0], x[1]\n",
    "        self_emb = self.self_linear(x_r)\n",
    "        alpha_i = self.attn_i(self_emb)\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), alpha=alpha_i, edge_attr=edge_attr, size=size)\n",
    "        out = self.lin_l(out) + self.lin_r(self_emb)  # dense layer.\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2., dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, alpha_i: Tensor, edge_attr: Tensor, index: Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> Tensor:\n",
    "        message = torch.cat([x_j, edge_attr], dim=-1)\n",
    "        out = self.message_linear(message)\n",
    "        alpha = self.attn(out) + alpha_i\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        self._alpha = alpha\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        out = out * alpha\n",
    "        return out\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "\n",
    "\n",
    "class CrossModalityAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-Modality Fusion in GraFrank implemented by an attention mechanism across the K modalities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(CrossModalityAttention, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.multi_linear = nn.Linear(hidden_channels, hidden_channels, bias=True)\n",
    "        self.multi_attn = nn.Sequential(self.multi_linear, nn.Tanh(), nn.Linear(hidden_channels, 1, bias=True))\n",
    "\n",
    "    def forward(self, modality_x_list):\n",
    "        \"\"\"\n",
    "\n",
    "        :param modality_x_list: list of modality-specific node embeddings.\n",
    "        :return: final node embedding after fusion.\n",
    "        \"\"\"\n",
    "        result = torch.cat([x.relu().unsqueeze(-2) for x in modality_x_list], -2)  # [...., K, hidden_channels]\n",
    "        wts = torch.softmax(self.multi_attn(result).squeeze(-1), dim=-1)\n",
    "        return torch.sum(wts.unsqueeze(-1) * self.multi_linear(result), dim=-2)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.hidden_channels,\n",
    "                                   self.hidden_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e1a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraFrank(nn.Module):\n",
    "    \"\"\"\n",
    "    GraFrank Model for Multi-Faceted Friend Ranking with multi-modal node features and pairwise link features.\n",
    "    (a) Modality-specific neighbor aggregation: modality_convs\n",
    "    (b) Cross-modality fusion layer: cross_modality_attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, edge_channels, num_layers, input_dim_list):\n",
    "        \"\"\"\n",
    "\n",
    "        :param in_channels: total cardinality of node features.\n",
    "        :param hidden_channels: latent embedding dimensionality.\n",
    "        :param edge_channels: number of link features.\n",
    "        :param num_layers: number of message passing layers.\n",
    "        :param input_dim_list: list containing the cardinality of node features per modality.\n",
    "        \"\"\"\n",
    "        super(GraFrank, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.modality_convs = nn.ModuleList()\n",
    "        self.edge_channels = edge_channels\n",
    "        # we assume that the input features are first partitioned and then concatenated across the K modalities.\n",
    "        self.input_dim_list = input_dim_list\n",
    "\n",
    "        for inp_dim in self.input_dim_list:\n",
    "            modality_conv_list = nn.ModuleList()\n",
    "            for i in range(num_layers):\n",
    "                in_channels = in_channels if i == 0 else hidden_channels\n",
    "                modality_conv_list.append(GraFrankConv((inp_dim + edge_channels, inp_dim), hidden_channels))\n",
    "\n",
    "            self.modality_convs.append(modality_conv_list)\n",
    "\n",
    "        self.cross_modality_attention = CrossModalityAttention(hidden_channels)\n",
    "\n",
    "    def forward(self, x, adjs, edge_attrs):\n",
    "        \"\"\" Compute node embeddings by recursive message passing, followed by cross-modality fusion.\n",
    "\n",
    "        :param x: node features [B', in_channels] where B' is the number of nodes (and neighbors) in the mini-batch.\n",
    "        :param adjs: list of sampled edge indices per layer (EdgeIndex format in PyTorch Geometric) in the mini-batch.\n",
    "        :param edge_attrs: [E', edge_channels] where E' is the number of sampled edge indices per layer in the mini-batch.\n",
    "        :return: node embeddings. [B, hidden_channels] where B is the number of target nodes in the mini-batch.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for k, convs_k in enumerate(self.modality_convs):\n",
    "            emb_k = None\n",
    "            for i, ((edge_index, _, size), edge_attr) in enumerate(zip(adjs, edge_attrs)):\n",
    "                x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "                x_list = torch.split(x, split_size_or_sections=self.input_dim_list, dim=-1)  # modality partition\n",
    "                x_target_list = torch.split(x_target, split_size_or_sections=self.input_dim_list, dim=-1)\n",
    "                x_k, x_target_k = x_list[k], x_target_list[k]\n",
    "\n",
    "                emb_k = convs_k[i]((x_k, x_target_k), edge_index, edge_attr=edge_attr)\n",
    "\n",
    "                if i != self.num_layers - 1:\n",
    "                    emb_k = emb_k.relu()\n",
    "                    emb_k = F.dropout(emb_k, p=0.5, training=self.training)\n",
    "\n",
    "            result.append(emb_k)\n",
    "        return self.cross_modality_attention(result)\n",
    "\n",
    "    def full_forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\" Auxiliary function to compute node embeddings for all nodes at once for small graphs.\n",
    "\n",
    "        :param x: node features [N, in_channels] where N is the total number of nodes in the graph.\n",
    "        :param edge_index: edge indices [2, E] where E is the total number of edges in the graph.\n",
    "        :param edge_attr: link features [E, edge_channels] across all edges in the graph.\n",
    "        :return: node embeddings. [N, hidden_channels] for all nodes in the graph.\n",
    "        \"\"\"\n",
    "        x_list = torch.split(x, split_size_or_sections=self.input_dim_list, dim=-1)  # modality partition\n",
    "        result = []\n",
    "        for k, convs_k in enumerate(self.modality_convs):\n",
    "            x_k = x_list[k]\n",
    "            emb_k = None\n",
    "            for i, conv in enumerate(convs_k):\n",
    "                emb_k = conv(x_k, edge_index, edge_attr=edge_attr)\n",
    "\n",
    "                if i != self.num_layers - 1:\n",
    "                    emb_k = emb_k.relu()\n",
    "                    emb_k = F.dropout(emb_k, p=0.5, training=self.training)\n",
    "\n",
    "            result.append(emb_k)\n",
    "#         print(result)\n",
    "        return self.cross_modality_attention(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b55f5ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.conv import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e61d5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else hidden_channels\n",
    "            self.convs.append(SAGEConv((in_channels, in_channels), hidden_channels))\n",
    "\n",
    "    def forward(self, x, adjs, edge_attrs):\n",
    "        for i, ((edge_index, _, size), edge_attr) in enumerate(zip(adjs, edge_attrs)):\n",
    "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "    def full_forward(self, x, edge_index, edge_attr):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc1a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'GraFrank'\n",
    "# n_edge_channels = 5\n",
    "if model_type == 'GraFrank':\n",
    "    model = GraFrank(data.num_node_features, hidden_channels=64, edge_channels=n_edge_channels, num_layers=2,\n",
    "                     input_dim_list=[224])  # input dim list assumes that the node features are first\n",
    "    # partitioned and then concatenated across the K modalities.\n",
    "else:\n",
    "    model = SAGE(data.num_node_features, hidden_channels=64, num_layers=2)\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x = data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66fda8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05162eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictionModel(nn.Module):\n",
    "    def __init__(self, num_users, embedding_dim):\n",
    "        super(LinkPredictionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, user_indices):\n",
    "        user_embeddings = self.embedding(user_indices)\n",
    "        scores = self.fc(user_embeddings)\n",
    "        return torch.sigmoid(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7fe1c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 96, 185, 141,  ...,  62, 177, 180])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yi-Chen\\anaconda3\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "#產生postiv&negativ edges\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "# Specify the ratio of edges to keep for training (e.g., 0.8 for 80%)\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Split the edges into train and test sets\n",
    "data_edge = train_test_split_edges(data_backup, val_ratio=0.8,test_ratio=0.1)\n",
    "print(data_edge.val_pos_edge_index[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6db738e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 54, 197, 335,  ..., 286, 303, 186])\n"
     ]
    }
   ],
   "source": [
    "print(data_edge.val_neg_edge_index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b709ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    print(model)\n",
    "    total_loss = 0\n",
    "    it = 0\n",
    "    num_users=348\n",
    "    for batch_size, n_id, adjs in loader:\n",
    "        it += 1\n",
    "        edge_attrs = [data.edge_attr[e_id] for (edge_index, e_id, size) in adjs]\n",
    "        adjs = [adj for adj in adjs]\n",
    "        edge_attrs = [edge_attr for edge_attr in edge_attrs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x[n_id], adjs, edge_attrs) #final user representations\n",
    "        print(out)\n",
    "#         embeds = out.tolist()\n",
    "#         embeds = tuple(embeds)\n",
    "#         print(embeds)\n",
    "\n",
    "        # Sample a direct neighbor as a positive example\n",
    "#         positive_indices = data_edge.val_pos_edge_index[1]  # Assuming the second dimension represents the target nodes\n",
    "#         positive_indices = random.choice(positive_example_indices)\n",
    "#         print(positive_indices[0])\n",
    "\n",
    "        # Sample a random node as a negative example\n",
    "#         all_nodes = torch.arange(data.num_nodes)  # Assuming 'data' has a property 'num_nodes' representing the total number of nodes\n",
    "#         negative_indices = data_edge.val_neg_edge_index[1]\n",
    "\n",
    "#         targets = torch.cat([torch.ones(len(positive_indices)), torch.zeros(len(negative_indices))])\n",
    "#         print(targets)\n",
    "#         loss = F.binary_cross_entropy(predictions.squeeze(), targets)\n",
    "\n",
    "#         # Perform backpropagation and update model parameters\n",
    "#         loss.backward()\n",
    "#         # optimizer.step()  # Uncomment if using an optimizer\n",
    "\n",
    "#         # Extract probabilities and threshold to determine predicted links\n",
    "#         predicted_links = (predictions > 0.5).squeeze().tolist()\n",
    "\n",
    "#         print(\"Predicted links:\", predicted_links)\n",
    "        \n",
    "        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
    "\n",
    "\n",
    "        # binary skipgram loss can be replaced with margin-based pairwise ranking loss.\n",
    "        pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
    "        neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
    "        loss = -pos_loss - neg_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * out.size(0)\n",
    "\n",
    "    return total_loss / data.num_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b71f6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraFrank(\n",
      "  (modality_convs): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): GraFrankConv((229, 224), 64)\n",
      "      (1): GraFrankConv((229, 224), 64)\n",
      "    )\n",
      "  )\n",
      "  (cross_modality_attention): CrossModalityAttention(64, 64)\n",
      ")\n",
      "tensor([[-0.0014,  0.0364,  0.0376,  ..., -0.0767,  0.1671,  0.0011],\n",
      "        [-0.0221,  0.0423,  0.0561,  ..., -0.0959,  0.1294,  0.0467],\n",
      "        [-0.0005,  0.0313,  0.0396,  ..., -0.0767,  0.1338, -0.0086],\n",
      "        ...,\n",
      "        [ 0.0136,  0.0611,  0.0784,  ..., -0.0711,  0.1433,  0.0302],\n",
      "        [-0.0050,  0.0633,  0.0032,  ..., -0.0931,  0.1294,  0.0190],\n",
      "        [-0.0138,  0.0587,  0.0085,  ..., -0.1033,  0.1658,  0.0208]],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8952/513898078.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8952/1385845799.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loader)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#         print(\"Predicted links:\", predicted_links)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "train(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcadedf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraFrank(\n",
      "  (modality_convs): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): GraFrankConv((229, 224), 64)\n",
      "      (1): GraFrankConv((229, 224), 64)\n",
      "    )\n",
      "  )\n",
      "  (cross_modality_attention): CrossModalityAttention(64, 64)\n",
      ")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2146 is out of bounds for dimension 0 with size 357",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3020/3168506224.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m51\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch: {epoch:03d}, Loss: {loss:.4f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3020/1385845799.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loader)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mit\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0medge_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madjs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0madjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0madj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madjs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0medge_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0medge_attr\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medge_attrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3020/1385845799.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mit\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0medge_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madjs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0madjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0madj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0madj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madjs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0medge_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0medge_attr\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0medge_attr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medge_attrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2146 is out of bounds for dimension 0 with size 357"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    x, edge_index, edge_attr = data.x.to(device), data.edge_index.to(device), data.edge_attr.to(device)\n",
    "    model.eval()\n",
    "    out = model.full_forward(x, edge_index, edge_attr).cpu()\n",
    "    return out\n",
    "\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    loss = train(train_loader)\n",
    "    test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a62e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6384a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe044c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65901343",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'adjs' and 'edge_attrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3020/831274779.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Compute predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Compute loss (e.g., binary cross entropy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'adjs' and 'edge_attrs'"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "num_users = 100\n",
    "embedding_dim = 64\n",
    "# model = LinkPredictionModel(num_users, embedding_dim)\n",
    "\n",
    "# Generate example data\n",
    "user_indices = data.edge_index[1]  # User indices for prediction\n",
    "positive_indices = data_edge.val_pos_edge_index  # Positive link indices\n",
    "negative_indices = data_edge.val_neg_edge_index  # Negative link indices\n",
    "\n",
    "# Compute predictions\n",
    "predictions = model(user_indices)\n",
    "\n",
    "# Compute loss (e.g., binary cross entropy)\n",
    "targets = torch.cat([torch.ones(len(positive_indices)), torch.zeros(len(negative_indices))])\n",
    "loss = F.binary_cross_entropy(predictions.squeeze(), targets)\n",
    "\n",
    "# Perform backpropagation and update model parameters\n",
    "loss.backward()\n",
    "# optimizer.step()  # Uncomment if using an optimizer\n",
    "\n",
    "# Extract probabilities and threshold to determine predicted links\n",
    "predicted_links = (predictions > 0.5).squeeze().tolist()\n",
    "\n",
    "print(\"Predicted links:\", predicted_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8ece8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_size):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, embed_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embed_size)\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embeds = self.user_embedding(user_indices)\n",
    "        item_embeds = self.item_embedding(item_indices)\n",
    "        return user_embeds, item_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96fa96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([341, 316, 249,  ..., 327, 322, 143])\n"
     ]
    }
   ],
   "source": [
    "print(data_edge.val_neg_edge_index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3932f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, embed_size, num_epochs, learning_rate):\n",
    "    num_users, num_items, user_indices, item_indices = preprocess_data(data)\n",
    "    \n",
    "    model = LightGCN(num_users, num_items, embed_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        user_embeds, item_embeds = model(user_indices, item_indices)\n",
    "        loss = criterion(user_embeds, item_embeds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd3329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9320af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.9780051708221436\n",
      "Epoch 11/100, Loss: 1.9451525211334229\n",
      "Epoch 21/100, Loss: 1.9128049612045288\n",
      "Epoch 31/100, Loss: 1.881006121635437\n",
      "Epoch 41/100, Loss: 1.8497785329818726\n",
      "Epoch 51/100, Loss: 1.8191258907318115\n",
      "Epoch 61/100, Loss: 1.7890429496765137\n",
      "Epoch 71/100, Loss: 1.759519100189209\n",
      "Epoch 81/100, Loss: 1.7305418252944946\n",
      "Epoch 91/100, Loss: 1.7020972967147827\n",
      "LightGCN(\n",
      "  (user_embedding): Embedding(348, 64)\n",
      "  (item_embedding): Embedding(348, 64)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def preprocess_data(data):\n",
    "    num_users = data.num_nodes\n",
    "    num_items = data.num_nodes\n",
    "    user_indices = data_edge.val_pos_edge_index\n",
    "    item_indices = data_edge.val_neg_edge_index\n",
    "    \n",
    "    return num_users, num_items, user_indices, item_indices\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "embed_size = 64\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "model = train_model(data, embed_size, num_epochs, learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f94b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832796a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83f9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493199f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf68847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b1b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd9ac91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=512, shuffle=True)\n",
    "for batch in loader:\n",
    "    batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 子圖怎麼合起來用\n",
    "# Train/test怎麼做資料集的分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09af424a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of Data objects\n",
    "data_list = [data1,data2,data3,data4,data5,data6,data7,data8,data9,data10]\n",
    "\n",
    "\n",
    "# combined_dataset = dataset.shuffle()\n",
    "# len(combined_dataset)\n",
    "\n",
    "\n",
    "# Create a DataLoader for the combined dataset\n",
    "loader = DataLoader(data_list, batch_size=32, shuffle=True) #shuffle()方法確保資料集被隨機打亂\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd963e1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_channels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15292/2878198186.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 创建图卷积层\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 进行图卷积操作，输出结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'in_channels' is not defined"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "conv = GCNConv(in_channels, out_channels)  # 创建图卷积层\n",
    "output = conv(data.x, data.edge_index)  # 进行图卷积操作，输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1afe4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從文件中讀取節點特徵向量\n",
    "def read_node_features(file_path):\n",
    "    with open(file_path) as file:\n",
    "        lines = file.readlines()\n",
    "        node_features = []\n",
    "        a=1\n",
    "        for line in lines:\n",
    "            # 根據文件格式擷取節點特徵向量\n",
    "            features = line.strip(str(a)).split()\n",
    "            features = [float(f) for f in features]\n",
    "            node_features.append(features)\n",
    "            a=a+1\n",
    "        return np.array(node_features)\n",
    "\n",
    "# 讀取節點特徵向量數據\n",
    "node_features = read_node_features(r'facebook\\0.feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88ff0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8b6b708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[307,  71],\n",
      "        [334, 252],\n",
      "        [ 27,  54],\n",
      "        ...,\n",
      "        [  0, 345],\n",
      "        [  0, 346],\n",
      "        [  0, 347]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgeo_index=subgraph_data.edge_index\n",
    "print(edgeo_index.t())\n",
    "subgraph_data.num_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
